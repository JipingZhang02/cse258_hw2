{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(fname):\n",
    "    for l in open(fname):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(parseData(\"beer_50000.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = data[:25000]\n",
    "dataValid = data[25000:37500]\n",
    "dataTest = data[37500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = [d['beer/ABV'] > 7 for d in dataTrain]\n",
    "yValid = [d['beer/ABV'] > 7 for d in dataValid]\n",
    "yTest = [d['beer/ABV'] > 7 for d in dataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryCounts = defaultdict(int)\n",
    "for d in data:\n",
    "    categoryCounts[d['beer/style']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [c for c in categoryCounts if categoryCounts[c] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "catID = dict(zip(list(categories),range(len(categories))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_column_names = [kname for kname in data[0] if ((\"review\" in kname) and isinstance(data[0][kname],float))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len = max(len(datum['review/text']) for datum in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat(datum, includeCat = True, includeReview = True, includeLength = True, ret_np_array = False):\n",
    "    res = list()\n",
    "    if includeCat:\n",
    "        catID_1hot_vector = [0.0]*(len(catID))\n",
    "        style_this = datum['beer/style']\n",
    "        if style_this in catID:\n",
    "            catID_1hot_vector[catID[style_this]]=1.0\n",
    "        res.extend(catID_1hot_vector)\n",
    "    if includeReview:\n",
    "        for review_col_name in review_column_names:\n",
    "            res.append(datum[review_col_name]/5.0)\n",
    "    if includeLength:\n",
    "        res.append(len(datum['review/text'])/max_text_len)\n",
    "    assert len(res)>0,f\"the feat function returns no feature for datum {datum}\"\n",
    "    if ret_np_array:\n",
    "        res = np.array(res,dtype=float)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lin_reg = linear_model.LinearRegression()\n",
    "# lin_reg.fit()\n",
    "# lin_reg.predict()\n",
    "\n",
    "def get_performance_info(y_actual,y_predict):\n",
    "    if not isinstance(y_actual,np.ndarray):\n",
    "        y_actual = np.array(y_actual)\n",
    "    y_actual = y_actual.reshape((-1,))\n",
    "    y_predict = y_predict.reshape((-1,))\n",
    "    TP = np.sum((y_actual == 1) & (y_predict == 1))\n",
    "    FP = np.sum((y_actual == 0) & (y_predict == 1))\n",
    "    TN = np.sum((y_actual == 0) & (y_predict == 0))\n",
    "    FN = np.sum((y_actual == 1) & (y_predict == 0))\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = FN / (TP + FN)\n",
    "    BER = 1 - (0.5 * (TPR + TNR))\n",
    "    return TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER\n",
    "\n",
    "def pipeline(reg, includeCat = True, includeReview = True, includeLength = True):\n",
    "    get_x_row = lambda datum:feat(datum,includeCat=includeCat,includeReview=includeReview,includeLength=includeLength)\n",
    "    get_all_x = lambda data:np.array(list(get_x_row(datum) for datum in data),dtype=float)\n",
    "    x_train = get_all_x(dataTrain)\n",
    "    x_valid = get_all_x(dataValid)\n",
    "    x_test = get_all_x(dataTest)\n",
    "    logisticRegModel = linear_model.LogisticRegression(class_weight=\"balanced\",penalty=\"l2\",C=reg)\n",
    "    logisticRegModel.fit(x_train,yTrain)\n",
    "    y_pred_valid = logisticRegModel.predict(x_valid)>=0.5\n",
    "    y_pred_test = logisticRegModel.predict(x_test)>=0.5\n",
    "    return logisticRegModel,get_performance_info(yValid,y_pred_valid)[-1],get_performance_info(yTest,y_pred_test)[-1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, validBER, testBER = pipeline(10, True, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = [validBER, testBER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "mod2, validBER2, testBER2 = pipeline(10, True, True, True)\n",
    "answers['Q2'] = [validBER2, testBER2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "best_c,best_model,min_ber_valid,ber_test = 0,None,1.0,1.0\n",
    "for c in [0.001, 0.01, 0.1, 1, 10]:\n",
    "    model,ber_valid,b_t_this = pipeline(c,True,True,True)\n",
    "    if ber_valid<min_ber_valid:\n",
    "        best_c = c\n",
    "        best_model = model\n",
    "        min_ber_valid = ber_valid\n",
    "        ber_test = b_t_this\n",
    "\n",
    "answers['Q3'] = [best_c,min_ber_valid,ber_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, validBER, testBER_noCat = pipeline(1.0,False,True,True)\n",
    "mod, validBER, testBER_noReview = pipeline(1.0,True,False,True)\n",
    "mod, validBER, testBER_noLength = pipeline(1.0,True,True,False)\n",
    "answers['Q4'] = [testBER_noCat, testBER_noReview, testBER_noLength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "\n",
    "header = f.readline()\n",
    "header = header.strip().split('\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "pairsSeen = set()\n",
    "\n",
    "for line in f:\n",
    "    fields = line.strip().split('\\t')\n",
    "    d = dict(zip(header, fields))\n",
    "    ui = (d['customer_id'], d['product_id'])\n",
    "    if ui in pairsSeen:\n",
    "        # print(\"Skipping duplicate user/item:\", ui)\n",
    "        continue\n",
    "    pairsSeen.add(ui)\n",
    "    d['star_rating'] = int(d['star_rating'])\n",
    "    d['helpful_votes'] = int(d['helpful_votes'])\n",
    "    d['total_votes'] = int(d['total_votes'])\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = dataset[:int(len(dataset)*0.9)]\n",
    "dataTest = dataset[int(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPerItem = defaultdict(set) # Maps an item to the users who rated it\n",
    "itemsPerUser = defaultdict(set) # Maps a user to the items that they rated\n",
    "itemNames = {} \n",
    "ratingDict = {} # (u,i)->r To retrieve a rating for a specific user/item pair\n",
    "timeDict = {} # (u,i)->t\n",
    "reviewsPerUser = defaultdict(list) # TODO: what is this?\n",
    "# testSetDatas = set()\n",
    "\n",
    "for d in dataTrain:\n",
    "    try:\n",
    "        item_id = d['product_id']\n",
    "        user_id = d['customer_id']\n",
    "        rating = d['star_rating']\n",
    "        item_name = d['product_title']\n",
    "        review_time = d['review_date']\n",
    "        usersPerItem[item_id].add(user_id)\n",
    "        itemsPerUser[user_id].add(item_id)\n",
    "        itemNames[item_id] = item_name\n",
    "        ratingDict[(user_id,item_id)]=rating\n",
    "        timeDict[(user_id,item_id)]=dateutil.parser.parse(review_time).timestamp()\n",
    "    except BaseException as e:\n",
    "        print(f\"error happened when dealing with {d} : str({e})\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSetDatas = set()\n",
    "testSetRatingDict = {}\n",
    "testSetTimeDict = {}\n",
    "\n",
    "for d in dataTest:\n",
    "    try:\n",
    "        item_id = d['product_id']\n",
    "        user_id = d['customer_id']\n",
    "        rating = d['star_rating']\n",
    "        item_name = d['product_title']\n",
    "        review_time = d['review_date']\n",
    "        testSetDatas.add((user_id,item_id))\n",
    "        itemNames[item_id] = item_name\n",
    "        testSetRatingDict[(user_id,item_id)]=rating\n",
    "        testSetTimeDict[(user_id,item_id)]=dateutil.parser.parse(review_time).timestamp()\n",
    "    except BaseException as e:\n",
    "        print(f\"error happened when dealing with {d} : str({e})\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "userAverages = {} # avg rating every user gives items he/she bought\n",
    "itemAverages = {} # avg rating of every item given by users who bought it\n",
    "\n",
    "for u in itemsPerUser:\n",
    "    total_score,item_cnt=0,0\n",
    "    for item_this_user_bought in itemsPerUser[u]:\n",
    "        # if (u,item_this_user_bought) in testSetDatas:\n",
    "        #     continue\n",
    "        total_score+=ratingDict[(u,item_this_user_bought)]\n",
    "        item_cnt+=1\n",
    "    if item_cnt==0:\n",
    "        continue\n",
    "    userAverages[u] = total_score/item_cnt\n",
    "    \n",
    "for i in usersPerItem:\n",
    "    total_score,user_cnt=0,0\n",
    "    for user_bought_this_item in usersPerItem[i]:\n",
    "        # if (user_bought_this_item,i) in testSetDatas:\n",
    "        #     continue\n",
    "        total_score+=ratingDict[(user_bought_this_item,i)]\n",
    "        user_cnt+=1\n",
    "    if user_cnt==0:\n",
    "        continue\n",
    "    itemAverages[i] = total_score/user_cnt\n",
    "\n",
    "ratingMean = sum(r for _k,r in ratingDict.items())/len(ratingDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Jaccard(item1, item2):\n",
    "#     u_i1_set = usersPerItem[item1]\n",
    "#     u_i2_set = usersPerItem[item2]\n",
    "#     return len(u_i1_set.intersection(u_i2_set))/len(u_i1_set.union(u_i2_set))\n",
    "\n",
    "def Jaccard(s1, s2):\n",
    "    if len(s1)+len(s2)==0:\n",
    "        return 0\n",
    "    return len(s1.intersection(s2))/len(s1.union(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostSimilar(i, N):\n",
    "    simWithItemId = []\n",
    "    u_i_set = usersPerItem[i]\n",
    "    for j,u_j_set in usersPerItem.items():\n",
    "        if j==i:\n",
    "            continue\n",
    "        sim_this = Jaccard(u_i_set,u_j_set)\n",
    "        simWithItemId.append((sim_this,j))\n",
    "    simWithItemId.sort(key=lambda tup:tup[0],reverse=True)\n",
    "    return simWithItemId[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'B00KCHRKD6'\n",
    "ms = mostSimilar(query, 10)\n",
    "answers['Q5'] = ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPERNIGHT 5050 16.4ft 5M RGBWW LED Strip\n",
      "5pcs Pack 10mm L-shape 4-conductor Quick Splitter Right Angle Corner Connector for 5050 RGB LED Strip Lights, Strip to Strip\n",
      "HitLights RGB LED Tape Light Strips\n",
      "Audio2000's 1/4\" TS To XLR Female Microphone Cable (2 Pack)\n",
      "HitLights RGB LED Tape Light Strips\n",
      "Crank Up Dj Light Stands (2 Pack) Stage Lighting Truss System by Griffin | Portable Speaker Tripod | Heavy Duty Standing Rig | Adjustable Height Trussing|Holds 6 Can Lights|Music Performance Equipment\n",
      "American Dj S-Hook S Clamp Hang And Tighten\n",
      "Donner 8pcs DMX512 DMX Dfi DJ 2.4G Wireless 6 Receiver & 2 Transmitter Lighting Control\n",
      "Unbreakable Rubber Mic Clip For Extra Large Microphones\n",
      "Lamplite 500 Watt Par 64 Par Lamp With Mogul Plug Medium Flood\n",
      "Cecilio CEVN Style 1 Silent Electric Solid Wood Violin with Ebony Fittings\n"
     ]
    }
   ],
   "source": [
    "# print(itemNames[query])\n",
    "# for _sim,item_id in ms:\n",
    "#     print(itemNames[item_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, ypred):\n",
    "    if not isinstance(y,np.ndarray):\n",
    "        y = np.array(y)\n",
    "    if not isinstance(ypred,np.ndarray):\n",
    "        ypred = np.array(ypred)\n",
    "    return np.sum((y-ypred)**2)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRating(user,item):\n",
    "    uISet = usersPerItem[item]\n",
    "    if item not in itemAverages:\n",
    "        return ratingMean\n",
    "    itemAvgRating = itemAverages[item]\n",
    "    deltaRatingWeightedSum,weightSum=0.0,0.0\n",
    "    for j in itemsPerUser[user]:\n",
    "        if j==item:\n",
    "            continue\n",
    "        similarity = Jaccard(uISet,usersPerItem[j])\n",
    "        deltaRatingWeightedSum+=similarity*(ratingDict[(user,j)]-itemAverages[j])\n",
    "        weightSum+=similarity\n",
    "    if weightSum==0.0:\n",
    "        return itemAvgRating\n",
    "    return itemAvgRating+deltaRatingWeightedSum/weightSum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "simPredictions = list(predictRating(u,i) for (u,i) in testSetDatas)\n",
    "labels = list(testSetRatingDict[ui_tuple] for ui_tuple in testSetDatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = MSE(simPredictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_times_in_train_set = list(map(lambda tup:tup[1],timeDict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_TIMES = 1000\n",
    "total_time_span = 0.0\n",
    "for _ in range(RAND_TIMES):\n",
    "    i = random.randint(0,len(all_times_in_train_set))\n",
    "    j = random.randint(0,len(all_times_in_train_set))\n",
    "    total_time_span += abs(all_times_in_train_set[i]-all_times_in_train_set[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_time_span = total_time_span/RAND_TIMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [01:23<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "def predictRatingWithTimeFactor(user,item,timeFactorFunc):\n",
    "    uISet = usersPerItem[item]\n",
    "    if item not in itemAverages:\n",
    "        return ratingMean\n",
    "    itemAvgRating = itemAverages[item]\n",
    "    deltaRatingWeightedSum,weightSum=0.0,0.0\n",
    "    for j in itemsPerUser[user]:\n",
    "        if j==item:\n",
    "            continue\n",
    "        similarity = Jaccard(uISet,usersPerItem[j])\n",
    "        weight = timeFactorFunc(user,item,j) * similarity\n",
    "        deltaRatingWeightedSum+=weight*(ratingDict[(user,j)]-itemAverages[j])\n",
    "        weightSum+=weight\n",
    "    if weightSum==0.0:\n",
    "        return itemAvgRating\n",
    "    return itemAvgRating+deltaRatingWeightedSum/weightSum\n",
    "\n",
    "USE_TQDM = True\n",
    "\n",
    "krange = range(-20,21)\n",
    "if USE_TQDM:\n",
    "    import tqdm\n",
    "    krange = tqdm.tqdm(krange)\n",
    "\n",
    "lowest_mse,best_lambda,best_k = 2000000000,None,None\n",
    "e=2.718281828\n",
    "for k in krange:\n",
    "    decay_lambda_inverse = (2**k)*avg_time_span\n",
    "    timeFactorFunc = lambda u,i,j:e**(-abs(timeDict[(u,j)]-testSetTimeDict[(u,i)])/decay_lambda_inverse)\n",
    "    simPredictions = list(predictRatingWithTimeFactor(u,i,timeFactorFunc) for (u,i) in testSetDatas)\n",
    "    labels = list(testSetRatingDict[ui_tuple] for ui_tuple in testSetDatas)\n",
    "    mse_this = MSE(simPredictions,labels)\n",
    "    if mse_this<lowest_mse:\n",
    "        lowest_mse = mse_this\n",
    "        best_k = k\n",
    "        best_lambda = 1/decay_lambda_inverse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6993407480416247"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [\"In this problem, I used the teacher's suggested timeFactorFunction, f(t)=exp(-lambda*t).\\n\"+\n",
    "                 \"To make it reasonable, lambda inverse should be comparable to time span between purchases\\n\"+\n",
    "                 \"So I first used random sampling to estimate the average time_span between purchases\\n\"+\n",
    "                 \"Then searched lambda inverse through [2^(-20)~2^(20)]*avg_time_span and see which has best effect\\n\"+\n",
    "                 \"Finally I found 2^(-14)*avg_time_span as lambda inverse is the best\\n\"+\n",
    "                 \"Which means we should give recent purchase a much bigger weight than earlier purchases.\\n\"\n",
    "                 , lowest_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw2.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
